{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepDream.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqYkTfhLhxQN",
        "colab_type": "text"
      },
      "source": [
        "# DeepDream\n",
        "\n",
        "Implemented By:\n",
        "<table align=\"left\">\n",
        "  <td> <h5> <b> Roee Zamir </b> </h5> </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.linkedin.com/in/roee-zamir-500121177/\"><img src=\"https://cdn3.iconfinder.com/data/icons/free-social-icons/67/linkedin_circle_color-512.png\" width=\"36\"/></a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/roeez\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /></a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/roeez/DeepDream/blob/master/DeepDream.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>  \n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCyN8jq9h3Ps",
        "colab_type": "text"
      },
      "source": [
        "**What?**</br>\n",
        "This notebook contains an unofficial minimal simplified PyTorch Implementation of [DeepDream](https://ai.googleblog.com/2015/07/deepdream-code-example-for-visualizing.html) (A. Mordvintsev 2015), a cool method to give your neural network LSD and create psychedelic images. \n",
        "\n",
        "**How?**</br>\n",
        "The idea in DeepDream is very simple, we choose one or more layers of a trained network, and we modify the input image to \"excite\" these layers (increase their response), By doing so iteratively the network enhances patterns it sees in the image, resulting in a dream-like image.\n",
        "\n",
        "**Why?**</br>\n",
        "Networks just wanna have fun\n",
        "\n",
        "![Example](https://github.com/roeez/DeepDream/raw/master/example/wis.gif)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbbWjoPH-UAE",
        "colab_type": "text"
      },
      "source": [
        "####Imports, defintions and utility functions.####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryNYboFv9XEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy\n",
        "import math\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import IPython.display as display\n",
        "\n",
        "class FeatureExtractor(torch.nn.Module):\n",
        "    '''Get all the features extracted by the conv layers'''\n",
        "    def __init__(self, model):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.model = model\n",
        "    \n",
        "    def forward(self, x):\n",
        "        feats = []\n",
        "        for l in self.model.children():\n",
        "            x = l(x)\n",
        "            if isinstance(l, torch.nn.Conv2d):\n",
        "                feats.append(x)\n",
        "        return feats\n",
        "\n",
        "class Clip(object):\n",
        "    '''Clip values transformation'''\n",
        "    def __init__(self, min, max):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return x.clamp(self.min, self.max)\n",
        "\n",
        "# ImageNet normalization\n",
        "mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "std = torch.tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "preprocess = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "postprocess = T.Compose([\n",
        "    T.Normalize(mean=-mean/std, std=1/std),\n",
        "    Clip(0,1),\n",
        "    T.ToPILImage(),\n",
        "])\n",
        "\n",
        "def get_ranges(image):\n",
        "    '''\n",
        "    Returns the ranges [min_val, max_val] of the intensities of each channel\n",
        "    of the input image (Used later for clipping of the modified image intensities)\n",
        "    '''\n",
        "    ranges = []\n",
        "    for i in range(3):\n",
        "        ranges.append([image[:,i,:].min().item(), image[:,i,:].max().item()])\n",
        "\n",
        "    return ranges\n",
        "\n",
        "def clip_intensities(image, ranges):\n",
        "    for i in range(3):\n",
        "        image[:,i,:,:] = image[:,i,:,:].clamp(*ranges[i])\n",
        "\n",
        "    return image\n",
        "\n",
        "def imshow(img):\n",
        "  display.display(Image.fromarray(np.array(img)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFmf_Q4VFG8y",
        "colab_type": "text"
      },
      "source": [
        "Now we will choose a trained classification model.</br>\n",
        "`torchvision` offers a nice [list](https://pytorch.org/docs/stable/torchvision/models.html#classification) of implemented and trained models.<br>\n",
        "Colab allows you to use GPU, use it! (Runtime->Change runtime type->GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz8vgISAYVRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dismiss the fully connected layers\n",
        "model = next(torchvision.models.vgg19(pretrained=True).children())\n",
        "model = FeatureExtractor(model)\n",
        "model.eval()\n",
        "\n",
        "# In this method, we don't change the model's weights. \n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq1LjjcvHHq7",
        "colab_type": "text"
      },
      "source": [
        "Download and open an example image to work with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn99aZvDcRnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/roeez/DeepDream/raw/master/example/wis.jpg\n",
        "\n",
        "IMG_PATH = '/content/wis.jpg'\n",
        "\n",
        "input_batch = preprocess(Image.open(IMG_PATH))[None, ...]\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIK22aj_nJ8Z",
        "colab_type": "text"
      },
      "source": [
        "# The core code #\n",
        "The original basic DeepDream algorithm takes as input a trained model, an input image, a chosen layer to extract features from, #iterations and learning rate. In this implementation i added suuport for a weighted multiple layers choice. Deeper layers respond to higher-level features as eyes or faces while earlier layers respond to low-level features as edges, shapes and textures.\n",
        "In each iteration we calculate the strength of the response of the chosen layers to the input image ($\\ell{_2}$ norm for e.g.) which will be increased using *gradient* ***ascent*** by calculating the gradient of the response with respect to the input image and adding it to the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMbdG9jMfGS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deepDream(model, image, layers, weights, iterations=10, lr=.01):\n",
        "    ''' The core of DeepDream\n",
        "    Params:\n",
        "    model (nn.Module)\n",
        "    image (Tensor)\n",
        "    layers (list): list of chosen layers number\n",
        "    weights (list): list of the weights of the layers\n",
        "    iterations (int)\n",
        "    lr (float): the learning rate\n",
        "    '''\n",
        "\n",
        "    ranges = get_ranges(image)\n",
        "\n",
        "    # Gradients calculated with respect to the input image\n",
        "    image = torch.autograd.Variable(image.clone(), requires_grad=True)\n",
        "\n",
        "    # iteratively increase the response of the chosen layers \n",
        "    for i in range(iterations):\n",
        "        # extract features from the selected layers\n",
        "        feats = map(model(image).__getitem__, layers)\n",
        "\n",
        "        loss = .0 # The strength of the layers' response to increase\n",
        "        for w, f in zip(weights, feats):\n",
        "            loss += w * f.norm() # You can try other response reductions\n",
        "        loss.backward()\n",
        "\n",
        "        # Normalize the gradients\n",
        "        grad = image.grad.data - image.grad.data.mean()\n",
        "        grad = grad / grad.std()\n",
        "\n",
        "        # Gradient ascent - we want to increase the response\n",
        "        image.data += lr * image.grad / image.grad.data.abs().mean()\n",
        "\n",
        "        clip_intensities(image.data, ranges)\n",
        "\n",
        "        image.grad.zero_()\n",
        "\n",
        "    return image.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6GcM1m4JNMI",
        "colab_type": "text"
      },
      "source": [
        "In order to produce multi-scale fine patterns we will apply the algorithm on a pyramid - multiple scales of the input image (referred to as octaves by the author)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er9suYrUpuiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image = input_batch.clone()\n",
        "original_size = torch.tensor(image.shape[-2:])\n",
        "SCALE_FACTOR = 1.4\n",
        "MIN_SCALE_POWER = -(np.floor(math.log(min(image.shape[-2:])/28, SCALE_FACTOR)).astype(np.int))\n",
        "MAX_SCALE_POWER = 1\n",
        "\n",
        "octaves = []\n",
        "\n",
        "for i in range(MIN_SCALE_POWER, MAX_SCALE_POWER):\n",
        "    octaves.append(torch.nn.functional.interpolate(image, size=(original_size*(SCALE_FACTOR**i)).int().tolist(), mode='bicubic'))\n",
        "\n",
        "details = torch.zeros_like(octaves[0])\n",
        "for octave_num, octave in enumerate(octaves):\n",
        "    image = octave + torch.nn.functional.interpolate(details, size=octave.shape[-2:],  mode='bicubic')\n",
        "    image = deepDream(model, image, [5, 7, 10], [.2, .2, .6], 20, .005)\n",
        "    details = image - octave\n",
        "    display.clear_output(wait=True)\n",
        "    imshow(postprocess(image[0].cpu()))\n",
        "\n",
        "image = torch.nn.functional.interpolate(image, size=original_size.int().tolist(), mode='bicubic')\n",
        "display.clear_output(wait=True)\n",
        "imshow(postprocess(image[0].cpu()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFw-sm4XP_zE",
        "colab_type": "text"
      },
      "source": [
        "# What's next? #\n",
        "Feel free to try different images, models, layers, weights, #iterations, learning rate etc.\n",
        "This implementation ommits some tricks that were used in the [original implementation](https://github.com/google/deepdream/blob/master/dream.ipynb) as random jittering to the input image and other tricks which i didn't find so useful. Another intresting suggestion is to use as objective the dot product the features of the image with features of other guide image to somewhat control the patterns.\n",
        "# Takeaways#\n",
        "*   Trained networks encapsulates patterns\n",
        "*   Deeper layers reponse to high-level features\n",
        "*   Earlier layers respose to low-level features\n",
        "\n",
        "## Links ##\n",
        "\n",
        "\n",
        "*   [Original blog post](https://ai.googleblog.com/2015/07/deepdream-code-example-for-visualizing.html)\n",
        "*   [Original blog post #2](https:///ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)\n",
        "*   [Official Caffe implementation](https://github.com/google/deepdream)\n",
        "*   [Official TensorFlow tutorial](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}